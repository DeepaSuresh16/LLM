{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_X9to_JxGSd7",
        "outputId": "0c2aefa9-ab61-46dd-bc6a-66c96804a6c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "ORIGINAL TEXT:\n",
            "AI is transforming Data Analysis. Don't forget your Oracle exam!\n",
            "------------------------------\n",
            "\n",
            "1. Python .split() (Basic Whitespace):\n",
            "['AI', 'is', 'transforming', 'Data', 'Analysis.', \"Don't\", 'forget', 'your', 'Oracle', 'exam!']\n",
            "\n",
            "2. NLTK WhitespaceTokenizer:\n",
            "['AI', 'is', 'transforming', 'Data', 'Analysis.', \"Don't\", 'forget', 'your', 'Oracle', 'exam!']\n",
            "\n",
            "3. NLTK Word Tokenizer (Handles punctuation):\n",
            "['AI', 'is', 'transforming', 'Data', 'Analysis', '.', 'Do', \"n't\", 'forget', 'your', 'Oracle', 'exam', '!']\n",
            "\n",
            "Note: Notice how in methods 1 & 2, 'Analysis.' includes the period.\n",
            "In method 3, the period is its own token. This is better for ML models!\n"
          ]
        }
      ],
      "source": [
        "# 1. Setup: Importing libraries\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import WhitespaceTokenizer, word_tokenize\n",
        "\n",
        "# Download the 'punkt' package which NLTK needs for its advanced tokenizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# 2. Define a sample text\n",
        "# Since you're interested in Power BI and AI, let's use a relevant example\n",
        "sample_text = \"AI is transforming Data Analysis. Don't forget your Oracle exam!\"\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(\"ORIGINAL TEXT:\")\n",
        "print(sample_text)\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# --- METHOD A: Basic Python Whitespace Split ---\n",
        "# This is the simplest form. It splits wherever it finds a space.\n",
        "method_a = sample_text.split()\n",
        "print(\"\\n1. Python .split() (Basic Whitespace):\")\n",
        "print(method_a)\n",
        "\n",
        "# --- METHOD B: NLTK Whitespace Tokenizer ---\n",
        "# Specifically looks for whitespace characters (tabs, newlines, spaces).\n",
        "ws_tokenizer = WhitespaceTokenizer()\n",
        "method_b = ws_tokenizer.tokenize(sample_text)\n",
        "print(\"\\n2. NLTK WhitespaceTokenizer:\")\n",
        "print(method_b)\n",
        "\n",
        "# --- METHOD C: NLTK Word Tokenizer (The \"Smarter\" way) ---\n",
        "# This is what most ML practitioners use because it separates punctuation.\n",
        "method_c = word_tokenize(sample_text)\n",
        "print(\"\\n3. NLTK Word Tokenizer (Handles punctuation):\")\n",
        "print(method_c)\n",
        "\n",
        "# 4. A quick look at \"Subword Tokenization\" (The logic behind LLMs)\n",
        "# This isn't whitespace-based, but it's how modern AI sees text\n",
        "print(\"\\nNote: Notice how in methods 1 & 2, 'Analysis.' includes the period.\")\n",
        "print(\"In method 3, the period is its own token. This is better for ML models!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "tokens = tokenizer.tokenize(sentences[0])\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "df = pd.DataFrame({\"Token\": tokens, \"ID\": ids})\n",
        "print(df)# Step 1: Install the Transformers library\n",
        "!pip install transformers -q\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Step 2: Initialize the Tokenizer\n",
        "# We'll use 'bert-base-uncased' as it's the standard for learning tokenization\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Step 3: Define your input text\n",
        "raw_text = \"Hugging Face is a giant in the AI community. Their tokenizers are incredibly fast!\"\n",
        "\n",
        "# --- SENTENCE-LEVEL HANDLING ---\n",
        "# In modern NLP, we usually split text into a list of strings\n",
        "# For high-precision sentence splitting, libraries like 'pysbd' or 'nltk' are used.\n",
        "# Here is a clean way to handle multiple sentences:\n",
        "sentences = [s.strip() + \".\" for s in raw_text.split(\".\") if s]\n",
        "\n",
        "print(f\"--- Step 1: Sentence Tokenization ---\")\n",
        "for i, s in enumerate(sentences):\n",
        "    print(f\"Sentence {i+1}: {s}\")\n",
        "\n",
        "# --- WORD/SUBWORD TOKENIZATION ---\n",
        "print(f\"\\n--- Step 2: Subword Tokenization & Encoding ---\")\n",
        "\n",
        "# We can process all sentences at once (Batch Processing)\n",
        "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "    # Convert text to human-readable subword tokens\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "\n",
        "    # Convert text to numerical IDs (what the model actually sees)\n",
        "    input_ids = tokenizer.encode(sentence)\n",
        "\n",
        "    print(f\"\\nResults for Sentence {i+1}:\")\n",
        "    print(f\"Tokens:  {tokens}\")\n",
        "    print(f\"IDs:     {input_ids}\")\n",
        "\n",
        "# --- DECODING ---\n",
        "print(f\"\\n--- Step 3: Decoding ---\")\n",
        "decoded = tokenizer.decode(inputs['input_ids'][0])\n",
        "print(f\"Decoded first sentence: {decoded}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAFl6D0pGTeL",
        "outputId": "a7a04d92-b53a-468e-fec0-8d741d53fbae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Token     ID\n",
            "0    hugging  17662\n",
            "1       face   2227\n",
            "2         is   2003\n",
            "3          a   1037\n",
            "4      giant   5016\n",
            "5         in   1999\n",
            "6        the   1996\n",
            "7         ai   9932\n",
            "8  community   2451\n",
            "9          .   1012\n",
            "--- Step 1: Sentence Tokenization ---\n",
            "Sentence 1: Hugging Face is a giant in the AI community.\n",
            "Sentence 2: Their tokenizers are incredibly fast!.\n",
            "\n",
            "--- Step 2: Subword Tokenization & Encoding ---\n",
            "\n",
            "Results for Sentence 1:\n",
            "Tokens:  ['hugging', 'face', 'is', 'a', 'giant', 'in', 'the', 'ai', 'community', '.']\n",
            "IDs:     [101, 17662, 2227, 2003, 1037, 5016, 1999, 1996, 9932, 2451, 1012, 102]\n",
            "\n",
            "Results for Sentence 2:\n",
            "Tokens:  ['their', 'token', '##izer', '##s', 'are', 'incredibly', 'fast', '!', '.']\n",
            "IDs:     [101, 2037, 19204, 17629, 2015, 2024, 11757, 3435, 999, 1012, 102]\n",
            "\n",
            "--- Step 3: Decoding ---\n",
            "Decoded first sentence: [CLS] hugging face is a giant in the ai community. [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oaOgm4eMLLHq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}